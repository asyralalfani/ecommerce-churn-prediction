{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Environment & Dataset Setup Test\n",
    "## E-commerce Customer Churn Prediction\n",
    "\n",
    "**Purpose:**\n",
    "- Verify all libraries are installed correctly\n",
    "- Test data loading capabilities\n",
    "- Check dataset integrity\n",
    "- Validate file system structure\n",
    "- Ensure visualization tools work\n",
    "\n",
    "**Run this notebook first before starting the project!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING LIBRARY IMPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "libraries = [\n",
    "    ('pandas', 'pd'),\n",
    "    ('numpy', 'np'),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('sklearn', 'sklearn'),\n",
    "    ('tensorflow', 'tf'),\n",
    "    ('plotly.express', 'px'),\n",
    "]\n",
    "\n",
    "import_results = []\n",
    "\n",
    "for lib_name, alias in libraries:\n",
    "    try:\n",
    "        exec(f\"import {lib_name} as {alias}\")\n",
    "        # Get version if available\n",
    "        try:\n",
    "            version = eval(f\"{alias}.__version__\")\n",
    "        except:\n",
    "            version = \"N/A\"\n",
    "        \n",
    "        import_results.append({\n",
    "            'Library': lib_name,\n",
    "            'Status': '‚úÖ SUCCESS',\n",
    "            'Version': version\n",
    "        })\n",
    "        print(f\"‚úÖ {lib_name:30s} | Version: {version}\")\n",
    "    except ImportError as e:\n",
    "        import_results.append({\n",
    "            'Library': lib_name,\n",
    "            'Status': '‚ùå FAILED',\n",
    "            'Version': str(e)\n",
    "        })\n",
    "        print(f\"‚ùå {lib_name:30s} | Error: {e}\")\n",
    "\n",
    "# Summary\n",
    "success_count = sum(1 for r in import_results if 'SUCCESS' in r['Status'])\n",
    "total_count = len(import_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if success_count == total_count:\n",
    "    print(f\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY ({success_count}/{total_count})\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  SOME LIBRARIES FAILED ({success_count}/{total_count})\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Data Science Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING DATA MANIPULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test DataFrame\n",
    "test_data = {\n",
    "    'A': np.random.randint(1, 100, 10),\n",
    "    'B': np.random.random(10),\n",
    "    'C': ['Category' + str(i) for i in range(10)]\n",
    "}\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"‚úÖ Created test DataFrame:\")\n",
    "print(df_test.head())\n",
    "\n",
    "print(f\"\\n‚úÖ DataFrame info:\")\n",
    "print(f\"   Shape: {df_test.shape}\")\n",
    "print(f\"   Columns: {df_test.columns.tolist()}\")\n",
    "print(f\"   Data types: {df_test.dtypes.tolist()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ NumPy array operations:\")\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"   Array: {arr}\")\n",
    "print(f\"   Mean: {arr.mean()}\")\n",
    "print(f\"   Sum: {arr.sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ DATA MANIPULATION TEST PASSED\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create test plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Line plot\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "axes[0].plot(x, y, color='blue', linewidth=2)\n",
    "axes[0].set_title('Test Plot 1: Sine Wave', fontweight='bold')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('sin(X)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bar plot\n",
    "categories = ['A', 'B', 'C', 'D', 'E']\n",
    "values = [23, 45, 56, 78, 32]\n",
    "axes[1].bar(categories, values, color='steelblue', edgecolor='black')\n",
    "axes[1].set_title('Test Plot 2: Bar Chart', fontweight='bold')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Matplotlib plots rendered successfully!\")\n",
    "\n",
    "# Test seaborn\n",
    "plt.figure(figsize=(8, 4))\n",
    "data = np.random.randn(100)\n",
    "sns.histplot(data, kde=True, color='coral')\n",
    "plt.title('Test Plot 3: Seaborn Histogram with KDE', fontweight='bold')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Seaborn plots rendered successfully!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ VISUALIZATION TEST PASSED\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING MACHINE LEARNING LIBRARIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test StandardScaler\n",
    "data = np.random.randn(100, 3)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(f\"‚úÖ StandardScaler: Mean ‚âà {scaled_data.mean():.4f}, Std ‚âà {scaled_data.std():.4f}\")\n",
    "\n",
    "# Test LabelEncoder\n",
    "labels = ['cat', 'dog', 'cat', 'bird', 'dog', 'bird']\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(labels)\n",
    "print(f\"‚úÖ LabelEncoder: {labels} ‚Üí {encoded.tolist()}\")\n",
    "\n",
    "# Test train_test_split\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"‚úÖ train_test_split: Train={X_train.shape}, Test={X_test.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ MACHINE LEARNING LIBRARIES TEST PASSED\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING TENSORFLOW/KERAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ Keras version: {keras.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úÖ GPU Available: {len(gpus)} GPU(s) detected\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  GPU Not Available (CPU mode)\")\n",
    "\n",
    "# Create simple model test\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, activation='relu', input_shape=(5,)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(f\"\\n‚úÖ Created test model:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ TENSORFLOW/KERAS TEST PASSED\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check File System Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING FILE SYSTEM STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define expected directories\n",
    "expected_dirs = [\n",
    "    '../data',\n",
    "    '../data/raw',\n",
    "    '../data/processed',\n",
    "    '../data/model',\n",
    "    '../notebooks',\n",
    "    '../src',\n",
    "]\n",
    "\n",
    "print(\"\\nChecking directories:\")\n",
    "for dir_path in expected_dirs:\n",
    "    exists = os.path.exists(dir_path)\n",
    "    status = \"‚úÖ\" if exists else \"‚ö†Ô∏è \"\n",
    "    print(f\"{status} {dir_path:30s} - {'EXISTS' if exists else 'NOT FOUND'}\")\n",
    "    \n",
    "    # Create if doesn't exist\n",
    "    if not exists:\n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"   ‚Üí Created directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Üí Error creating directory: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ FILE SYSTEM CHECK COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for dataset in data/raw/\n",
    "data_path = '../data/raw/'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"\\n‚úÖ Found {len(csv_files)} CSV file(s):\")\n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join(data_path, file)\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   - {file:40s} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Try to load first CSV file\n",
    "        print(f\"\\nüìä Loading dataset: {csv_files[0]}\")\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_path, csv_files[0]))\n",
    "            print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "            print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "            print(f\"   Memory: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "            print(f\"\\n   Columns: {df.columns.tolist()}\")\n",
    "            print(f\"\\n   First 3 rows:\")\n",
    "            display(df.head(3))\n",
    "            \n",
    "            print(f\"\\n   Data types:\")\n",
    "            print(df.dtypes)\n",
    "            \n",
    "            print(f\"\\n   Missing values:\")\n",
    "            missing = df.isnull().sum()\n",
    "            if missing.sum() > 0:\n",
    "                print(missing[missing > 0])\n",
    "            else:\n",
    "                print(\"   ‚úÖ No missing values!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error loading dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No CSV files found in {data_path}\")\n",
    "        print(f\"\\nüì• Please download dataset:\")\n",
    "        print(f\"   1. Download from Kaggle or use converted CSV\")\n",
    "        print(f\"   2. Place in: {data_path}\")\n",
    "        print(f\"   3. Filename: ecommerce_data.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Data directory not found: {data_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ DATASET CHECK COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. System Resources Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SYSTEM RESOURCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# CPU Info\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "cpu_percent = psutil.cpu_percent(interval=1)\n",
    "print(f\"\\nüíª CPU:\")\n",
    "print(f\"   Cores: {cpu_count}\")\n",
    "print(f\"   Usage: {cpu_percent}%\")\n",
    "\n",
    "# Memory Info\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"\\nüß† Memory:\")\n",
    "print(f\"   Total: {memory.total / (1024**3):.2f} GB\")\n",
    "print(f\"   Available: {memory.available / (1024**3):.2f} GB\")\n",
    "print(f\"   Used: {memory.percent}%\")\n",
    "\n",
    "# Disk Info\n",
    "disk = psutil.disk_usage('/')\n",
    "print(f\"\\nüíæ Disk:\")\n",
    "print(f\"   Total: {disk.total / (1024**3):.2f} GB\")\n",
    "print(f\"   Free: {disk.free / (1024**3):.2f} GB\")\n",
    "print(f\"   Used: {disk.percent}%\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìä RECOMMENDATIONS:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if memory.available / (1024**3) < 2:\n",
    "    print(\"‚ö†Ô∏è  Low memory available. Consider closing other applications.\")\n",
    "else:\n",
    "    print(\"‚úÖ Sufficient memory available for ML tasks\")\n",
    "\n",
    "if disk.free / (1024**3) < 5:\n",
    "    print(\"‚ö†Ô∏è  Low disk space. Consider freeing up space.\")\n",
    "else:\n",
    "    print(\"‚úÖ Sufficient disk space available\")\n",
    "\n",
    "if cpu_count < 2:\n",
    "    print(\"‚ö†Ô∏è  Limited CPU cores. Training may be slow.\")\n",
    "else:\n",
    "    print(f\"‚úÖ {cpu_count} CPU cores available for parallel processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Setup Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ SETUP TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = [\n",
    "    (\"Python Environment\", \"‚úÖ READY\"),\n",
    "    (\"Core Libraries\", \"‚úÖ READY\"),\n",
    "    (\"Visualization Tools\", \"‚úÖ READY\"),\n",
    "    (\"Machine Learning Libraries\", \"‚úÖ READY\"),\n",
    "    (\"TensorFlow/Keras\", \"‚úÖ READY\"),\n",
    "    (\"File System Structure\", \"‚úÖ READY\"),\n",
    "]\n",
    "\n",
    "# Check dataset status\n",
    "dataset_status = \"‚úÖ READY\" if os.path.exists('../data/raw/ecommerce_data.csv') else \"‚ö†Ô∏è  PENDING\"\n",
    "summary.append((\"Dataset\", dataset_status))\n",
    "\n",
    "print(\"\\nComponent Status:\")\n",
    "for component, status in summary:\n",
    "    print(f\"  {component:30s} | {status}\")\n",
    "\n",
    "all_ready = all(\"READY\" in status for _, status in summary)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if all_ready:\n",
    "    print(\"‚úÖ ALL SYSTEMS GO! Ready to start the project!\")\n",
    "    print(\"\\nüìö Next Steps:\")\n",
    "    print(\"   1. Run: 01_data_preprocessing.ipynb\")\n",
    "    print(\"   2. Run: 02_exploratory_data_analysis.ipynb\")\n",
    "    print(\"   3. Continue with model building\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  SOME COMPONENTS NEED ATTENTION\")\n",
    "    print(\"\\nüìù Action Items:\")\n",
    "    for component, status in summary:\n",
    "        if \"PENDING\" in status or \"FAILED\" in status:\n",
    "            print(f\"   - Fix: {component}\")\n",
    "            \n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTest completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüöÄ Happy ML Engineering!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
